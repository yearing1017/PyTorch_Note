# PyTorch_Note
â°PyTorchå­¦ä¹ ç¬”è®°
## ğŸ’¡ 1. Pytorch_tutorial
- [Pytorch_60min.md](https://github.com/yearing1017/PyTorch_Note/blob/master/Pytorch_60min.md)ï¼šå®˜æ–¹60åˆ†é’Ÿå…¥é—¨Pytorch
- [Pytorch_Basic.py](https://github.com/yearing1017/PyTorch_Note/blob/master/Pytorch_Basic.py)ï¼šè‡ªåŠ¨æ±‚å¯¼ã€æ•°æ®é›†çš„ä½¿ç”¨ã€æ¨¡å‹ä¿å­˜åŠè½½å…¥
- [Pytorch_linearRegression.py](https://github.com/yearing1017/PyTorch_Note/blob/master/Pytorch_linearRegression.py)ï¼šçº¿æ€§å›å½’ä¾‹å­å®ç°å®Œæ•´è®­ç»ƒ
- [Pytorch_logisticRegression.py](https://github.com/yearing1017/PyTorch_Note/blob/master/Pytorch_logisticRegression.py)ï¼šMINIST+é€»è¾‘å›å½’å®ç°è®­ç»ƒæµ‹è¯•
- [Pytorch_NNdemo.py](https://github.com/yearing1017/PyTorch_Note/blob/master/Pytorch_NNdemo.py)ï¼šMINIST+ç®€æ˜“ç¥ç»ç½‘ç»œå®ç°è®­ç»ƒæµ‹è¯•
- [Pytorch_CNN](https://github.com/yearing1017/PyTorch_Note/blob/master/Pytorch_CNN.py)ï¼šMINST+å·ç§¯ç¥ç»ç½‘ç»œè®­ç»ƒæµ‹è¯•
- [pytorch_cuda.ipynb](https://github.com/yearing1017/PyTorch_Note/blob/master/pytorch_cuda.ipynb)ï¼špytorchæœ‰å…³cudaçš„åŸºæœ¬æ“ä½œä¸æ¦‚å¿µ
- [LeNet.ipynb](https://github.com/yearing1017/PyTorch_Note/blob/master/LeNet.ipynb)ï¼špytorchæ­å»ºLeNetç½‘ç»œ
- [ResNet.ipynb](https://github.com/yearing1017/PyTorch_Note/blob/master/ResNet.ipynb)ï¼špytorchæ­å»ºResNet
- [Pytorch_å›¾åƒå¢å¼º](https://github.com/yearing1017/PyTorch_Note/blob/master/Pytorch_å›¾åƒå¢å¼º.md)ï¼šæ€»ç»“äº†pytorchä¸­ä¸»è¦ç”¨åˆ°çš„7ä¸­å›¾åƒå¢å¼ºçš„æ–¹æ³•
- [DenseNet_PyTorchå®ç°](https://github.com/yearing1017/PyTorch_Note/blob/master/DenseNet_PyTorch.md)ï¼›å›é¡¾DenseNetçš„æ ¸å¿ƒæ¶æ„ä»¥åŠä½¿ç”¨PyTorchè¿›è¡Œå®ç°
- [PyTorchä¿å­˜æ¨¡å‹ä¸¤ç§æ–¹å¼çš„æ¯”è¾ƒ](https://zhuanlan.zhihu.com/p/94971100)ï¼šä¿å­˜æ¨¡å‹å’Œä¿å­˜æ¨¡å‹å‚æ•°åŠloadä½¿ç”¨çš„æ–¹å¼
- [PyTorchå¯¹äºæ•°æ®é›†çš„å¤„ç†æ–¹å¼-torch.utils.data](https://www.cnblogs.com/Bella2017/p/11791216.html)ï¼šsubsetæ ¹æ®ç´¢å¼•æ¥è·å–å­é›†
- [PyTorchå¸¸ç”¨ä»£ç æ®µè½¬è½½](https://zhuanlan.zhihu.com/p/104019160)ï¼šéå¸¸é«˜äººæ°”çš„æ–‡ç« ï¼Œé€‚åˆæŸ¥é˜…

## ğŸ’¡ 2. Pytorch_å·²è§£å†³é—®é¢˜_1

- åœ¨è·‘unetçš„æ¨¡å‹æ—¶ï¼Œé‡åˆ°è¯¥é”™è¯¯:

`RuntimeError: Given groups=1, weight of size 64 3 3 3, expected input[4, 64, 158, 158] to have 3 channels, but got 64 channels instead`

- é—®é¢˜æ˜¯è¾“å…¥æœ¬æ¥è¯¥æ˜¯ 3 channelsï¼Œä½†å´æ˜¯64é€šé“ã€‚

- è§£å†³æ€è·¯ï¼šæ‰“å°äº†ä¸€ä¸‹è¾“å…¥çš„size:[4,3,160,160],æœ¬æ¥ä»¥ä¸ºæ²¡é”™è¯¯ï¼Œå°±ä¸€ç›´åœ¨æ‰¾ã€‚

- å®é™…é—®é¢˜ï¼šå› ä¸ºæˆ‘åœ¨ä»¥ä¸‹ä»£ç éƒ¨åˆ†æœ‰ä¸¤ä¸ªå·ç§¯æ“ä½œï¼Œæˆ‘çš„ç¬¬äºŒä¸ªå·ç§¯çš„è¾“å…¥åº”è¯¥æ˜¯ç¬¬ä¸€ä¸ªå·ç§¯çš„è¾“å‡ºï¼Œæˆ‘å´è®¾å®šäº†ä¸¤è€…ç›¸åŒã€‚å¦‚ä¸‹ï¼š
```python
class DoubleConv(nn.Module):
	def __init__(self, in_channels, out_channels):
		super.__init__()
		# æ„å»ºä¸€ä¸ªâ€œå®¹å™¨ç½‘ç»œâ€
		self.double_conv = nn.Sequential(
			nn.Conv2d(in_channels,out_channels,kernel_size=3),
			nn.BatchNorm2d(out_channels),
			nn.ReLU(inplace=True),
			nn.Conv2d(in_channels,out_channels,kernel_size=3),
			nn.BatchNorm2d(out_channels),
			nn.ReLU(inplace=True)
		)
	def forward(self,x):
		return self.double_conv(x)
```

- åœ¨ç¬¬25è¡Œçš„å·ç§¯ä¸­ï¼Œæˆ‘çš„in_channelså’Œç¬¬ä¸€ä¸ªå·ç§¯çš„ä¸€æ ·ï¼Œä½†å´åº”è¯¥æ˜¯ç¬¬ä¸€ä¸ªçš„è¾“å‡ºï¼Œæ‰€ä»¥æ”¹ä¸ºout_channels,å¦‚ä¸‹ï¼š
```python
class DoubleConv(nn.Module):
	def __init__(self, in_channels, out_channels):
		super.__init__()
		# æ„å»ºä¸€ä¸ªâ€œå®¹å™¨ç½‘ç»œâ€
		self.double_conv = nn.Sequential(
			nn.Conv2d(in_channels,out_channels,kernel_size=3),
			nn.BatchNorm2d(out_channels),
			nn.ReLU(inplace=True),
			nn.Conv2d(out_channels,out_channels,kernel_size=3),
			nn.BatchNorm2d(out_channels),
			nn.ReLU(inplace=True)
		)
	def forward(self,x):
		return self.double_conv(x)
```

## ğŸ’¡ 3. Pytorch_cv2_Tensorç›¸å…³

- ä¸€ä¸ªç°åº¦çš„å›¾ç‰‡ï¼Œåªæœ‰ä¸€ä¸ªé€šé“ï¼Œåªæ˜¯cv2è¯»å–imageï¼Œæ‰“å°shapeï¼Œä»…ä»…æ˜¾ç¤º[H, W]

- è‹¥ä½¿ç”¨äº†torchvision.ToTensor()æ–¹æ³•ï¼Œå†æ‰“å°shapeï¼Œä¼šæ‰“å°å‡º[C, H, W],ä¸”è¿›è¡Œäº†å½’ä¸€åŒ–ï¼Œå–å€¼èŒƒå›´ä¸º[0,1.0]çš„torch.FloatTensor
```python
import cv2
import torch
import torchvision.transforms

image_name = 'Dataset/2d_images/ID_0000_Z_0142.tif'
image = cv2.imread(image_name, 0)
print(image.shape) # (512,512)
image_tensor = torchvision.transforms.ToTensor()(image)
print(image_tensor.shape) # torch.Size([1, 512, 512])
```

- è¿˜æœ‰ä¸€ç§æš´åŠ›æ–¹æ³•å¾—åˆ°æƒ³è¦çš„shapeï¼Œå…ˆresizeï¼Œå†reshapeï¼Œæœ€åå†è½¬ä¸ºtensorï¼Œè¿™æœŸé—´æ²¡æœ‰è¿›è¡Œå½’ä¸€åŒ–
```python
import torch
import torchvision.transforms
import cv2

image_name = 'Dataset/2d_images/ID_0000_Z_0142.tif'
image = cv2.imread(image_name, 0)
print(image.shape)
image = cv2.resize(image, (160, 160))
image_new = image.reshape((1, 160, 160))
image_tensor = torch.FloatTensor(image_new)
print(image_new.shape)
print(image_tensor.shape)

# è¾“å‡ºï¼š
(512, 512)
(1, 160, 160)
torch.Size([1, 160, 160])
```
- `cv2.resize(img, (width, height))`å‚æ•°æ˜¯:å…ˆå®½åé«˜

- **class torchvision.transforms.ToTensor**:
  - æŠŠä¸€ä¸ªå–å€¼èŒƒå›´æ˜¯`[0,255]`çš„`PIL.Image`æˆ–è€…`shape`ä¸º`(H,W,C)`çš„`numpy.ndarray`ï¼Œè½¬æ¢æˆå½¢çŠ¶ä¸º`[C,H,W]`ï¼Œå–å€¼èŒƒå›´æ˜¯`[0,1.0]`çš„`torch.FloatTensor`
  
- **class torchvision.transforms.Normalize(mean, std)**:
  - ç»™å®šå‡å€¼ï¼š`(R,G,B)` æ–¹å·®ï¼š`ï¼ˆRï¼ŒGï¼ŒBï¼‰`ï¼Œå°†ä¼šæŠŠ`Tensor`æ­£åˆ™åŒ–ã€‚å³ï¼š`Normalized_image=(image-mean)/std`
  
- cv2.imread(img, 1)ï¼šè¿”å›ç»“æœä¸º`type: numpy.ndarray `ï¼Œå¤šç»´æ•°ç»„

## ğŸ’¡ 4. Pytorchæ¨¡å‹æ„é€ 

###  4.1 ç»§æ‰¿Moduleç±»æ¥æ„é€ æ¨¡å‹

- è¿™é‡Œå®šä¹‰çš„MLPç±»é‡è½½äº†Moduleç±»çš„__init__å‡½æ•°å’Œforwardå‡½æ•°ã€‚å®ƒä»¬åˆ†åˆ«ç”¨äºåˆ›å»ºæ¨¡å‹å‚æ•°å’Œå®šä¹‰å‰å‘è®¡ç®—ã€‚å‰å‘è®¡ç®—ä¹Ÿå³æ­£å‘ä¼ æ’­ã€‚
```python
import torch
from torch import nn

class MLP(nn.Module):
    # å£°æ˜å¸¦æœ‰æ¨¡å‹å‚æ•°çš„å±‚ï¼Œè¿™é‡Œå£°æ˜äº†ä¸¤ä¸ªå…¨è¿æ¥å±‚
    def __init__(self, **kwargs):
        # è°ƒç”¨MLPçˆ¶ç±»Moduleçš„æ„é€ å‡½æ•°æ¥è¿›è¡Œå¿…è¦çš„åˆå§‹åŒ–ã€‚è¿™æ ·åœ¨æ„é€ å®ä¾‹æ—¶è¿˜å¯ä»¥æŒ‡å®šå…¶ä»–å‡½æ•°
        # å‚æ•°ï¼Œå¦‚â€œæ¨¡å‹å‚æ•°çš„è®¿é—®ã€åˆå§‹åŒ–å’Œå…±äº«â€ä¸€èŠ‚å°†ä»‹ç»çš„æ¨¡å‹å‚æ•°params
        super(MLP, self).__init__(**kwargs)
        self.hidden = nn.Linear(784, 256) # éšè—å±‚
        self.act = nn.ReLU()
        self.output = nn.Linear(256, 10)  # è¾“å‡ºå±‚


    # å®šä¹‰æ¨¡å‹çš„å‰å‘è®¡ç®—ï¼Œå³å¦‚ä½•æ ¹æ®è¾“å…¥xè®¡ç®—è¿”å›æ‰€éœ€è¦çš„æ¨¡å‹è¾“å‡º
    def forward(self, x):
        a = self.act(self.hidden(x))
        return self.output(a)
```

###  4.2 ä½¿ç”¨Sequentialç±»å®šä¹‰æ¨¡å‹

- å®ƒå¯ä»¥æ¥æ”¶ä¸€ä¸ªå­æ¨¡å—çš„æœ‰åºå­—å…¸ï¼ˆOrderedDictï¼‰æˆ–è€…ä¸€ç³»åˆ—å­æ¨¡å—ä½œä¸ºå‚æ•°æ¥é€ä¸€æ·»åŠ Moduleçš„å®ä¾‹
- æ¨¡å‹çš„å‰å‘è®¡ç®—å°±æ˜¯å°†è¿™äº›å®ä¾‹æŒ‰æ·»åŠ çš„é¡ºåºé€ä¸€è®¡ç®—
```python
import torch
from torch import nn
class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size
            nn.Sigmoid(),
            nn.MaxPool2d(2, 2), # kernel_size, stride
            nn.Conv2d(6, 16, 5),# in_channels, out_channels, kernel_size
            nn.Sigmoid(),
            nn.MaxPool2d(2, 2)
        )
        self.fc = nn.Sequential(
            nn.Linear(16*4*4, 120),
            nn.Sigmoid(),
            nn.Linear(120, 84),
            nn.Sigmoid(),
            nn.Linear(84, 10)
        )
    
    def forward(self, img):
        feature = self.conv(img)
        # viewç›¸å½“äºreshapeï¼Œè¿™é‡Œçš„img.shape[0]æ˜¯batch_sizeï¼Œ-1ä»£è¡¨è‡ªåŠ¨è®¡ç®—å‡ºæ¥çš„H*W*Channels
        output = self.fc(feature.view(img.shape[0], -1))
        return output
```

### 4.3 ModuleListç±»

- ModuleListæ¥æ”¶ä¸€ä¸ªå­æ¨¡å—çš„åˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œç„¶åç±»ä¼¼Listé‚£æ ·è¿›è¡Œappendå’Œextendæ“ä½œ:
```python
net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])
net.append(nn.Linear(256, 10)) # # ç±»ä¼¼Listçš„appendæ“ä½œ
print(net[-1])  # ç±»ä¼¼Listçš„ç´¢å¼•è®¿é—®
print(net)
# net(torch.zeros(1, 784)) # ä¼šæŠ¥NotImplementedError
# è¾“å‡º
Linear(in_features=256, out_features=10, bias=True)
ModuleList(
  (0): Linear(in_features=784, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=10, bias=True)
)
```
- æ—¢ç„¶Sequentialå’ŒModuleListéƒ½å¯ä»¥è¿›è¡Œåˆ—è¡¨åŒ–æ„é€ ç½‘ç»œï¼Œé‚£äºŒè€…åŒºåˆ«æ˜¯ä»€ä¹ˆå‘¢ã€‚
- ModuleListä»…ä»…æ˜¯ä¸€ä¸ªå‚¨å­˜å„ç§æ¨¡å—çš„åˆ—è¡¨ï¼Œè¿™äº›æ¨¡å—ä¹‹é—´æ²¡æœ‰è”ç³»ä¹Ÿæ²¡æœ‰é¡ºåºï¼ˆæ‰€ä»¥ä¸ç”¨ä¿è¯ç›¸é‚»å±‚çš„è¾“å…¥è¾“å‡ºç»´åº¦åŒ¹é…ï¼‰ï¼Œè€Œä¸”æ²¡æœ‰å®ç°forwardåŠŸèƒ½éœ€è¦è‡ªå·±å®ç°ï¼Œæ‰€ä»¥ä¸Šé¢æ‰§è¡Œnet(torch.zeros(1, 784))ä¼šæŠ¥NotImplementedErrorï¼›
- è€ŒSequentialå†…çš„æ¨¡å—éœ€è¦æŒ‰ç…§é¡ºåºæ’åˆ—ï¼Œè¦ä¿è¯ç›¸é‚»å±‚çš„è¾“å…¥è¾“å‡ºå¤§å°ç›¸åŒ¹é…ï¼Œå†…éƒ¨forwardåŠŸèƒ½å·²ç»å®ç°ã€‚

### 4.4 ModuleDictç±»
- ModuleDictæ¥æ”¶ä¸€ä¸ªå­æ¨¡å—çš„å­—å…¸ä½œä¸ºè¾“å…¥, ç„¶åä¹Ÿå¯ä»¥ç±»ä¼¼å­—å…¸é‚£æ ·è¿›è¡Œæ·»åŠ è®¿é—®æ“ä½œ:
```python
net = nn.ModuleDict({
    'linear': nn.Linear(784, 256),
    'act': nn.ReLU(),
})
net['output'] = nn.Linear(256, 10) # æ·»åŠ 
print(net['linear']) # è®¿é—®
print(net.output)
print(net)
# net(torch.zeros(1, 784)) # ä¼šæŠ¥NotImplementedError
# è¾“å‡º
Linear(in_features=784, out_features=256, bias=True)
Linear(in_features=256, out_features=10, bias=True)
ModuleDict(
  (act): ReLU()
  (linear): Linear(in_features=784, out_features=256, bias=True)
  (output): Linear(in_features=256, out_features=10, bias=True)
)
```
- å’ŒModuleListä¸€æ ·ï¼ŒModuleDictå®ä¾‹ä»…ä»…æ˜¯å­˜æ”¾äº†ä¸€äº›æ¨¡å—çš„å­—å…¸ï¼Œå¹¶æ²¡æœ‰å®šä¹‰forwardå‡½æ•°éœ€è¦è‡ªå·±å®šä¹‰ã€‚
- åŒæ ·ï¼ŒModuleDictä¹Ÿä¸Pythonçš„Dictæœ‰æ‰€ä¸åŒï¼ŒModuleDicté‡Œçš„æ‰€æœ‰æ¨¡å—çš„å‚æ•°ä¼šè¢«è‡ªåŠ¨æ·»åŠ åˆ°æ•´ä¸ªç½‘ç»œä¸­ã€‚

## ğŸ’¡ 5. Pytorchçš„CrossEntropyLoss

- é”™è¯¯æè¿°ï¼šè¯­ä¹‰åˆ†å‰²å®éªŒä¸­ï¼Œåœ¨å¯¹labelè¿›è¡Œonehotç¼–ç ä¹‹åï¼Œå°†å…¶å˜ä¸º(4,4,640,640)ï¼Œå®šä¹‰losså¦‚ä¸‹ï¼š
```python
criterion = nn.CrossEntropyLoss().to(device)
loss = criterion(output, label)
```
- æŠ¥é”™ï¼š**å¤§è‡´ä¸ºï¼ŒæœŸæœ›çš„targetæ˜¯3ç»´ï¼Œå´å¾—åˆ°äº†ä¸€ä¸ª4ç»´ã€‚**
- æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£å¦‚ä¸‹ï¼š
![](https://github.com/yearing1017/PyTorch_Note/blob/master/image/5-4.png)
- è¯¥æŸå¤±å‡½æ•°åŒ…å«äº†**softmaxå‡½æ•°**ï¼Œ**è¯¥æŸå¤±å‡½æ•°æœŸæœ›çš„targetæ˜¯åœ¨åƒç´ å€¼ä¸ºï¼ˆ0ï¼ŒC-1ï¼‰çš„ä¸€ä¸ªæ ‡æ³¨å›¾ã€‚ä¸æ ‡æ³¨å¥½çš„labelç›¸å¯¹åº”ï¼Œæ¯ä¸ªåƒç´ å€¼æ ‡æ³¨äº†ç±»åˆ«0-3ã€‚å…±4ç±»**
![](https://github.com/yearing1017/PyTorch_Note/blob/master/image/5-3.png)
- ä¸Šå›¾è¯¦ç»†è§£é‡Šäº†losså‡½æ•°çš„è¦æ±‚çš„shapeã€‚å¯¹äºè¯­ä¹‰åˆ†å‰²çš„4ç»´å‘é‡æ¥è¯´ï¼š**è¦æ±‚inputå³ç½‘ç»œçš„é¢„æµ‹ä¸º(N,C,H,W)ï¼Œtargetä¸º(N, H, W)ï¼Œä¸”target[i]åœ¨0-C-1ä¹‹é—´ã€‚**
- **æ”¹åŠ¨ï¼šå»æ‰onehotï¼Œç›´æ¥è¯»å…¥æ ‡æ³¨çš„labelï¼Œå› ä¸ºç¬¦åˆä¸Šè¿°è¦æ±‚ã€‚**

## ğŸ’¡ 6. tensorboardXçš„ç®€å•ä½¿ç”¨
- å®‰è£…ï¼š
`pip install tensorboardX`
- ç®€å•æ•°æ®çš„è®°å½•: `writer.add_scalar(åç§°ï¼Œæ•°å€¼ï¼Œxè½´åæ ‡)`
```python
from tensorboardX import SummaryWriter
writer.add_scalar('train_loss', train_loss/len(train_dataloader), epo)
```

## ğŸ’¡ 7. é¢„è®­ç»ƒæ¨¡å‹å‚æ•°çš„ä½¿ç”¨
- åœ¨è®­ç»ƒæ—¶ï¼Œä¼šè€ƒè™‘æ˜¯å¦é‡‡ç”¨åœ¨ä¾‹å¦‚Imageæ•°æ®é›†ä¸Šé¢„è®­ç»ƒå¾—åˆ°çš„å‚æ•°ï¼Œä½†æ˜¯æœ‰çš„æ—¶å€™é¢„è®­ç»ƒå¾—åˆ°çš„ç½‘ç»œç»“æ„æ˜¯å½“å‰è®­ç»ƒç½‘ç»œçš„ä¸€éƒ¨åˆ†
- ä¾‹å¦‚ï¼šdeeplabv3-resnetåŸºäºæ®‹å·®ç½‘ç»œï¼Œæˆ‘ä»¬è‹¥ä½¿ç”¨resnetçš„é¢„è®­ç»ƒå‚æ•°ï¼Œåˆ™éœ€è¦åˆ¤æ–­é‚£äº›å±‚å¯ä»¥ä½¿ç”¨ï¼ˆå…¶ä¸­deeplabv3ä¸­æ–°æ·»åŠ äº†asppå’Œç§»é™¤äº†FCï¼‰
- å®ä¾‹ä»£ç å¦‚ä¸‹ï¼š
```python
model_urls = {
    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',
    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',
    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',
}

# åŸºäºResNetçš„deeplabv3
class ResNet(nn.Module):
    def __init__(self, block, block_num, num_classes, num_groups=None, weight_std=False, beta=False, pretrained=False):
        self.inplanes = 64 # æ§åˆ¶æ®‹å·®å—çš„è¾“å…¥é€šé“æ•° planes:è¾“å‡ºé€šé“æ•°
        # nn.BatchNorm2då’Œnn.GroupNormä¸¤ç§ä¸åŒçš„å½’ä¸€åŒ–æ–¹æ³•
        self.norm = nn.BatchNorm2d
        self.conv = Conv2d if weight_std else nn.Conv2d
        super(ResNet, self).__init__()

        if not beta:
            # æ•´ä¸ªResNetçš„ç¬¬ä¸€ä¸ªconv
            self.conv1 = self.conv(3, 64, kernel_size=7, stride=2, padding=3,
                                   bias=False)
        else:
            # ç¬¬ä¸€ä¸ªæ®‹å·®æ¨¡å—çš„conv
            self.conv1 = nn.Sequential(
                self.conv(3, 64, 3, stride=2, padding=1, bias=False),
                self.conv(64, 64, 3, stride=1, padding=1, bias=False),
                self.conv(64, 64, 3, stride=1, padding=1, bias=False))
        self.bn1 = self.norm(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # å»ºç«‹æ®‹å·®å—éƒ¨åˆ†
        self.layer1 = self._make_layer(block, 64,  block_num[0])
        self.layer2 = self._make_layer(block, 128, block_num[1], stride=2)
        self.layer3 = self._make_layer(block, 256, block_num[2], stride=2)
        # block4å¼€å§‹ä¸ºdilationç©ºæ´å·ç§¯
        self.layer4 = self._make_layer(block, 512, block_num[3], stride=1, dilation=2)
        # aspp,512 * block.expansionæ˜¯ç»è¿‡æ®‹å·®æ¨¡å—çš„è¾“å‡ºé€šé“æ•°
        self.aspp = ASPP(512 * block.expansion, 256, num_classes, conv=self.conv, norm=self.norm)
        # éå†æ¨¡å‹è¿›è¡Œåˆå§‹åŒ–
        for m in self.modules():
            if isinstance(m, self.conv):        #isinstanceï¼šmç±»å‹åˆ¤æ–­    è‹¥å½“å‰ç»„ä»¶ä¸º conv
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))  #æ­£å¤ªåˆ†å¸ƒåˆå§‹åŒ–
            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.GroupNorm): #è‹¥ä¸ºbatchnorm
                m.weight.data.fill_(1)          #weightä¸º1
                m.bias.data.zero_()             #biasä¸º0

        if pretrained:
            self._load_pretrained_model()

    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):
        downsample = None
        # stride!=1 ä»£è¡¨åç»­æ®‹å·®å—ä¸­æœ‰stride=2ï¼Œå°ºå¯¸å¤§å°æ”¹å˜ï¼Œæ‰€ä»¥ç¬¬ä¸€ä¸ªæ®‹å·®å—ä¸­çš„strideä¹Ÿè¯¥ç”¨æ¥ä¿®æ”¹å°ºå¯¸
        if stride != 1 or dilation != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                self.conv(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, dilation=max(1, dilation/2), bias=False),
                self.norm(planes * block.expansion),
            )
        # laysers å­˜æ”¾äº§ç”Ÿçš„æ®‹å·®å—ï¼Œæœ€åæ ¹æ®æ­¤åˆ—è¡¨è¿›è¡Œç”Ÿæˆç½‘ç»œ
        layers = []
        # åœ¨å¤šä¸ªæ®‹å·®å—ä¸­ï¼Œåªæœ‰ç¬¬ä¸€ä¸ªæ®‹å·®å—çš„è¾“å…¥è¾“å‡ºé€šé“ä¸ä¸€è‡´ï¼Œæ‰€ä»¥å…ˆå•ç‹¬æ·»åŠ å¸¦downsampleçš„block
        layers.append(block(self.inplanes, planes, stride, downsample, dilation=max(1, dilation/2), conv=self.conv, norm=self.norm))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes, dilation=dilation, conv=self.conv, norm=self.norm))

        return nn.Sequential(*layers)


    def forward(self, x):
        # x.shape:[batch_size, channels, H, w]
        size = (x.shape[2], x.shape[3])
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        # ASPP
        x = self.aspp(x)
        #x = x.reshape(-1, x.shape[1])
        x = nn.Upsample(size, mode='bilinear', align_corners=True)(x)
        return x
    # æ ¹æ®å…·ä½“çš„ç½‘ç»œå±‚æ¥è½½å…¥æ¨¡å‹å‚æ•°    
    def _load_pretrained_model(self):
        pretrain_dict = model_zoo.load_url(model_urls['resnet152'])
        model_dict = {}
        state_dict = self.state_dict()
        for k, v in pretrain_dict.items():
            if k in state_dict:
                model_dict[k] = v
        state_dict.update(model_dict)
        self.load_state_dict(state_dict)

def resnet152(pretrained=True, **kwargs):
    """Constructs a ResNet-152 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 8, 36, 3], num_classes=4, pretrained=pretrained, **kwargs)
    return model
```
