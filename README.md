# PyTorch_Note
â°PyTorchå­¦ä¹ ç¬”è®°
## ğŸ’¡ Pytorch_tutorial
- [Pytorch_Basic.py](https://github.com/yearing1017/PyTorch_Note/blob/master/Pytorch_Basic.py)ï¼šè‡ªåŠ¨æ±‚å¯¼ã€æ•°æ®é›†çš„ä½¿ç”¨ã€æ¨¡å‹ä¿å­˜åŠè½½å…¥
- [Pytorch_linearRegression.py](https://github.com/yearing1017/PyTorch_Note/blob/master/Pytorch_linearRegression.py)ï¼šçº¿æ€§å›å½’ä¾‹å­å®ç°å®Œæ•´è®­ç»ƒ
- [Pytorch_logisticRegression.py](https://github.com/yearing1017/PyTorch_Note/blob/master/Pytorch_logisticRegression.py)ï¼šMINIST+é€»è¾‘å›å½’å®ç°è®­ç»ƒæµ‹è¯•
- [Pytorch_NNdemo.py](https://github.com/yearing1017/PyTorch_Note/blob/master/Pytorch_NNdemo.py)ï¼šMINIST+ç®€æ˜“ç¥ç»ç½‘ç»œå®ç°è®­ç»ƒæµ‹è¯•
- [Pytorch_CNN](https://github.com/yearing1017/PyTorch_Note/blob/master/Pytorch_CNN.py)ï¼šMINST+å·ç§¯ç¥ç»ç½‘ç»œè®­ç»ƒæµ‹è¯•
- [pytorch_cuda.ipynb](https://github.com/yearing1017/PyTorch_Note/blob/master/pytorch_cuda.ipynb)ï¼špytorchæœ‰å…³cudaçš„åŸºæœ¬æ“ä½œä¸æ¦‚å¿µ
- [LeNet.ipynb](https://github.com/yearing1017/PyTorch_Note/blob/master/LeNet.ipynb)ï¼špytorchæ­å»ºLeNetç½‘ç»œ
- [ResNet.ipynb](https://github.com/yearing1017/PyTorch_Note/blob/master/ResNet.ipynb)ï¼špytorchæ­å»ºResNet
## ğŸ’¡ Pytorch_å·²è§£å†³é—®é¢˜_1

- åœ¨è·‘unetçš„æ¨¡å‹æ—¶ï¼Œé‡åˆ°è¯¥é”™è¯¯:

`RuntimeError: Given groups=1, weight of size 64 3 3 3, expected input[4, 64, 158, 158] to have 3 channels, but got 64 channels instead`

- é—®é¢˜æ˜¯è¾“å…¥æœ¬æ¥è¯¥æ˜¯ 3 channelsï¼Œä½†å´æ˜¯64é€šé“ã€‚

- è§£å†³æ€è·¯ï¼šæ‰“å°äº†ä¸€ä¸‹è¾“å…¥çš„size:[4,3,160,160],æœ¬æ¥ä»¥ä¸ºæ²¡é”™è¯¯ï¼Œå°±ä¸€ç›´åœ¨æ‰¾ã€‚

- å®é™…é—®é¢˜ï¼šå› ä¸ºæˆ‘åœ¨ä»¥ä¸‹ä»£ç éƒ¨åˆ†æœ‰ä¸¤ä¸ªå·ç§¯æ“ä½œï¼Œæˆ‘çš„ç¬¬äºŒä¸ªå·ç§¯çš„è¾“å…¥åº”è¯¥æ˜¯ç¬¬ä¸€ä¸ªå·ç§¯çš„è¾“å‡ºï¼Œæˆ‘å´è®¾å®šäº†ä¸¤è€…ç›¸åŒã€‚å¦‚ä¸‹ï¼š
```python
class DoubleConv(nn.Module):
	def __init__(self, in_channels, out_channels):
		super.__init__()
		# æ„å»ºä¸€ä¸ªâ€œå®¹å™¨ç½‘ç»œâ€
		self.double_conv = nn.Sequential(
			nn.Conv2d(in_channels,out_channels,kernel_size=3),
			nn.BatchNorm2d(out_channels),
			nn.ReLU(inplace=True),
			nn.Conv2d(in_channels,out_channels,kernel_size=3),
			nn.BatchNorm2d(out_channels),
			nn.ReLU(inplace=True)
		)
	def forward(self,x):
		return self.double_conv(x)
```

- åœ¨ç¬¬25è¡Œçš„å·ç§¯ä¸­ï¼Œæˆ‘çš„in_channelså’Œç¬¬ä¸€ä¸ªå·ç§¯çš„ä¸€æ ·ï¼Œä½†å´åº”è¯¥æ˜¯ç¬¬ä¸€ä¸ªçš„è¾“å‡ºï¼Œæ‰€ä»¥æ”¹ä¸ºout_channels,å¦‚ä¸‹ï¼š
```python
class DoubleConv(nn.Module):
	def __init__(self, in_channels, out_channels):
		super.__init__()
		# æ„å»ºä¸€ä¸ªâ€œå®¹å™¨ç½‘ç»œâ€
		self.double_conv = nn.Sequential(
			nn.Conv2d(in_channels,out_channels,kernel_size=3),
			nn.BatchNorm2d(out_channels),
			nn.ReLU(inplace=True),
			nn.Conv2d(out_channels,out_channels,kernel_size=3),
			nn.BatchNorm2d(out_channels),
			nn.ReLU(inplace=True)
		)
	def forward(self,x):
		return self.double_conv(x)
```

## ğŸ’¡ Pytorch_cv2_Tensorç›¸å…³

- ä¸€ä¸ªç°åº¦çš„å›¾ç‰‡ï¼Œåªæœ‰ä¸€ä¸ªé€šé“ï¼Œåªæ˜¯cv2è¯»å–imageï¼Œæ‰“å°shapeï¼Œä»…ä»…æ˜¾ç¤º[H, W]

- è‹¥ä½¿ç”¨äº†torchvision.ToTensor()æ–¹æ³•ï¼Œå†æ‰“å°shapeï¼Œä¼šæ‰“å°å‡º[C, H, W],ä¸”è¿›è¡Œäº†å½’ä¸€åŒ–ï¼Œå–å€¼èŒƒå›´ä¸º[0,1.0]çš„torch.FloatTensor
```python
import cv2
import torch
import torchvision.transforms

image_name = 'Dataset/2d_images/ID_0000_Z_0142.tif'
image = cv2.imread(image_name, 0)
print(image.shape) # (512,512)
image_tensor = torchvision.transforms.ToTensor()(image)
print(image_tensor.shape) # torch.Size([1, 512, 512])
```

- è¿˜æœ‰ä¸€ç§æš´åŠ›æ–¹æ³•å¾—åˆ°æƒ³è¦çš„shapeï¼Œå…ˆresizeï¼Œå†reshapeï¼Œæœ€åå†è½¬ä¸ºtensorï¼Œè¿™æœŸé—´æ²¡æœ‰è¿›è¡Œå½’ä¸€åŒ–
```python
import torch
import torchvision.transforms
import cv2

image_name = 'Dataset/2d_images/ID_0000_Z_0142.tif'
image = cv2.imread(image_name, 0)
print(image.shape)
image = cv2.resize(image, (160, 160))
image_new = image.reshape((1, 160, 160))
image_tensor = torch.FloatTensor(image_new)
print(image_new.shape)
print(image_tensor.shape)

# è¾“å‡ºï¼š
(512, 512)
(1, 160, 160)
torch.Size([1, 160, 160])
```
- `cv2.resize(img, (width, height))`å‚æ•°æ˜¯:å…ˆå®½åé«˜

- **class torchvision.transforms.ToTensor**:
  - æŠŠä¸€ä¸ªå–å€¼èŒƒå›´æ˜¯`[0,255]`çš„`PIL.Image`æˆ–è€…`shape`ä¸º`(H,W,C)`çš„`numpy.ndarray`ï¼Œè½¬æ¢æˆå½¢çŠ¶ä¸º`[C,H,W]`ï¼Œå–å€¼èŒƒå›´æ˜¯`[0,1.0]`çš„`torch.FloatTensor`
  
- **class torchvision.transforms.Normalize(mean, std)**:
  - ç»™å®šå‡å€¼ï¼š`(R,G,B)` æ–¹å·®ï¼š`ï¼ˆRï¼ŒGï¼ŒBï¼‰`ï¼Œå°†ä¼šæŠŠ`Tensor`æ­£åˆ™åŒ–ã€‚å³ï¼š`Normalized_image=(image-mean)/std`
  
- cv2.imread(img, 1)ï¼šè¿”å›ç»“æœä¸º`type: numpy.ndarray `ï¼Œå¤šç»´æ•°ç»„

## ğŸ’¡ 60åˆ†é’Ÿç†Ÿæ‚‰Pytorch
- æœ¬éƒ¨åˆ†ä¸ºå®˜æ–¹çš„ä¸­æ–‡æ–‡æ¡£å†…å®¹ï¼Œæ”¾åœ¨é¦–é¡µä¸ºäº†æ¯æ¬¡æ–¹ä¾¿æŸ¥é˜…
### å¼ é‡

- `Tensor`ï¼ˆå¼ é‡ï¼‰ç±»ä¼¼äº`NumPy`çš„`ndarray`ï¼Œä½†è¿˜å¯ä»¥åœ¨GPUä¸Šä½¿ç”¨æ¥åŠ é€Ÿè®¡ç®—

```python
from __future__ import print_function
import torch
```

- åˆ›å»ºä¸€ä¸ªæ²¡æœ‰åˆå§‹åŒ–çš„5*3çŸ©é˜µï¼š

```python
x = torch.empty(5, 3)
print(x)
# è¾“å‡ºç»“æœ
'''
tensor([[2.2391e-19, 4.5869e-41, 1.4191e-17],
        [4.5869e-41, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00]])
'''
```

- åˆ›å»ºä¸€ä¸ªéšæœºåˆå§‹åŒ–çŸ©é˜µï¼š

```python
x = torch.rand(5, 3)
print(x)

# è¾“å‡ºç»“æœ
tensor([[0.5307, 0.9752, 0.5376],
        [0.2789, 0.7219, 0.1254],
        [0.6700, 0.6100, 0.3484],
        [0.0922, 0.0779, 0.2446],
        [0.2967, 0.9481, 0.1311]])
```

- æ„é€ ä¸€ä¸ªå¡«æ»¡`0`ä¸”æ•°æ®ç±»å‹ä¸º`long`çš„çŸ©é˜µ:

```python
x = torch.zeros(5, 3, dtype=torch.long)
print(x)

# è¾“å‡ºç»“æœ
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
```

- ç›´æ¥ä»æ•°æ®æ„é€ å¼ é‡ï¼š

```python
x = torch.tensor([5.5, 3])
print(x)

# è¾“å‡ºç»“æœ
tensor([5.5000, 3.0000])
```

- æ ¹æ®å·²æœ‰çš„tensorå»ºç«‹æ–°çš„tensorã€‚é™¤éç”¨æˆ·æä¾›æ–°çš„å€¼ï¼Œå¦åˆ™è¿™äº›æ–¹æ³•å°†é‡ç”¨è¾“å…¥å¼ é‡çš„å±æ€§ï¼Œä¾‹å¦‚dtypeç­‰ï¼š

```python
x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes
print(x)

x = torch.randn_like(x, dtype=torch.float)    # é‡è½½ dtype
print(x)                                      # ç»“æœsizeä¸€è‡´

# è¾“å‡ºç»“æœ
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[ 1.6040, -0.6769,  0.0555],
        [ 0.6273,  0.7683, -0.2838],
        [-0.7159, -0.5566, -0.2020],
        [ 0.6266,  0.3566,  1.4497],
        [-0.8092, -0.6741,  0.0406]])
```

- è·å–å¼ é‡çš„å½¢çŠ¶ï¼š

```python
print(x.size())
# torch.Size([5, 3])
```

### è¿ç®—

- ä»¥åŠ æ³•ä¸ºä¾‹ï¼š

```python
y = torch.rand(5, 3)
print(x + y)

# è¾“å‡º
tensor([[ 2.5541,  0.0943,  0.9835],
        [ 1.4911,  1.3117,  0.5220],
        [-0.0078, -0.1161,  0.6687],
        [ 0.8176,  1.1179,  1.9194],
        [-0.3251, -0.2236,  0.7653]])

# åŠ æ³•å½¢å¼äºŒ
print(torch.add(x, y))

# è¾“å‡º
tensor([[ 2.5541,  0.0943,  0.9835],
        [ 1.4911,  1.3117,  0.5220],
        [-0.0078, -0.1161,  0.6687],
        [ 0.8176,  1.1179,  1.9194],
        [-0.3251, -0.2236,  0.7653]])
```

- åŠ æ³•ï¼šç»™å®šä¸€ä¸ªè¾“å‡ºå¼ é‡ä½œä¸ºå‚æ•°

```python
result = torch.empty(5, 3)
torch.add(x, y, out=result)
print(result)

# è¾“å‡º
tensor([[ 2.5541,  0.0943,  0.9835],
        [ 1.4911,  1.3117,  0.5220],
        [-0.0078, -0.1161,  0.6687],
        [ 0.8176,  1.1179,  1.9194],
        [-0.3251, -0.2236,  0.7653]])
```

- åŠ æ³•ï¼šåŸä½/åŸåœ°æ“ä½œï¼ˆin-placeï¼‰

```python
# adds x to y
y.add_(x)
print(y)

# è¾“å‡º
tensor([[ 2.5541,  0.0943,  0.9835],
        [ 1.4911,  1.3117,  0.5220],
        [-0.0078, -0.1161,  0.6687],
        [ 0.8176,  1.1179,  1.9194],
        [-0.3251, -0.2236,  0.7653]])
```

> æ³¨æ„ï¼š
>
> ä»»ä½•ä¸€ä¸ªin-placeæ”¹å˜å¼ é‡çš„æ“ä½œåé¢éƒ½å›ºå®šä¸€ä¸ª`_`ã€‚ä¾‹å¦‚`x.copy_(y)`ã€`x.t_()`å°†æ›´æ”¹x

- ä¹Ÿå¯ä»¥ä½¿ç”¨åƒæ ‡å‡†çš„NumPyä¸€æ ·çš„å„ç§ç´¢å¼•æ“ä½œï¼š

```python
print(x[:, 1])

# è¾“å‡º
tensor([-0.6769,  0.7683, -0.5566,  0.3566, -0.6741])
```

- æ”¹å˜å½¢çŠ¶ï¼šå¦‚æœæƒ³æ”¹å˜å½¢çŠ¶ï¼Œå¯ä»¥ä½¿ç”¨`torch.view`

```python
x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  # the size -1 è¡¨ç¤ºæ ¹æ®å…¶ä»–çš„æ•°å€¼æ¥è®¡ç®—è¯¥ä½ç½®çš„æ•°å€¼
print(x.size(), y.size(), z.size())

# è¾“å‡º
torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
```

- å¦‚æœæ˜¯ä»…åŒ…å«ä¸€ä¸ªå…ƒç´ çš„tensorï¼Œå¯ä»¥ä½¿ç”¨`.item()`æ¥å¾—åˆ°å¯¹åº”çš„pythonæ•°å€¼

```python
x = torch.randn(1)
print(x)
print(x.item())

# è¾“å‡º
tensor([0.0445])
0.0445479191839695
```

> åç»­é˜…è¯»ï¼š
>
> è¶…è¿‡100ç§tensorçš„è¿ç®—æ“ä½œï¼ŒåŒ…æ‹¬è½¬ç½®ï¼Œç´¢å¼•ï¼Œåˆ‡ç‰‡ï¼Œæ•°å­¦è¿ç®—ï¼Œ çº¿æ€§ä»£æ•°ï¼Œéšæœºæ•°ç­‰ï¼Œå…·ä½“è®¿é—®[è¿™é‡Œ](https://pytorch.org/docs/stable/torch.html)

### æ¡¥æ¥Numpy

- ä¸€ä¸ªTorchå¼ é‡ä¸ä¸€ä¸ªNumPyæ•°ç»„çš„è½¬æ¢å¾ˆç®€å•

- Torchå¼ é‡å’ŒNumPyæ•°ç»„å°†å…±äº«å®ƒä»¬çš„åº•å±‚å†…å­˜ä½ç½®ï¼Œå› æ­¤å½“ä¸€ä¸ªæ”¹å˜æ—¶,å¦å¤–ä¹Ÿä¼šæ”¹å˜ã€‚

```python
a = torch.ones(5)
print(a)

# è¾“å‡º
tensor([1., 1., 1., 1., 1.])

b = a.numpy()
print(b)

# è¾“å‡º
[1. 1. 1. 1. 1.]

a.add_(1)
print(a)
print(b)

# è¾“å‡º
tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]
```

- numpyæ•°ç»„è½¬æ¢ä¸ºå¼ é‡

```python
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
np.add(a, 1, out=a)
print(a)
print(b)

# è¾“å‡º
[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
```

- CPUä¸Šçš„æ‰€æœ‰å¼ é‡(CharTensoré™¤å¤–)éƒ½æ”¯æŒä¸Numpyçš„ç›¸äº’è½¬æ¢ã€‚
- å¼ é‡å¯ä»¥ä½¿ç”¨`.to`æ–¹æ³•ç§»åŠ¨åˆ°ä»»ä½•è®¾å¤‡ï¼ˆdeviceï¼‰ä¸Šï¼š

```python
# å½“GPUå¯ç”¨æ—¶,æˆ‘ä»¬å¯ä»¥è¿è¡Œä»¥ä¸‹ä»£ç 
# æˆ‘ä»¬å°†ä½¿ç”¨`torch.device`æ¥å°†tensorç§»å…¥å’Œç§»å‡ºGPU
if torch.cuda.is_available():
    device = torch.device("cuda")          # a CUDA device object
    y = torch.ones_like(x, device=device)  # ç›´æ¥åœ¨GPUä¸Šåˆ›å»ºtensor
    x = x.to(device)                       # æˆ–è€…ä½¿ç”¨`.to("cuda")`æ–¹æ³•
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))       # `.to`ä¹Ÿèƒ½åœ¨ç§»åŠ¨æ—¶æ”¹å˜dtype
    
# è¾“å‡º
tensor([1.0445], device='cuda:0')
tensor([1.0445], dtype=torch.float64)
```

### è‡ªåŠ¨æ±‚å¯¼

#### å¼ é‡

- PyTorchä¸­ï¼Œæ‰€æœ‰ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ˜¯ `autograd` åŒ…ã€‚
- `autograd` åŒ…ä¸ºå¼ é‡ä¸Šçš„æ‰€æœ‰æ“ä½œæä¾›äº†è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶ã€‚å®ƒæ˜¯ä¸€ä¸ªåœ¨è¿è¡Œæ—¶å®šä¹‰ï¼ˆdefine-by-runï¼‰çš„æ¡†æ¶ï¼Œè¿™æ„å‘³ç€åå‘ä¼ æ’­æ˜¯æ ¹æ®ä»£ç å¦‚ä½•è¿è¡Œæ¥å†³å®šçš„ï¼Œå¹¶ä¸”æ¯æ¬¡è¿­ä»£å¯ä»¥æ˜¯ä¸åŒçš„ã€‚
- çœ‹ä¾‹å­
- `torch.Tensor` æ˜¯è¿™ä¸ªåŒ…çš„æ ¸å¿ƒç±»ã€‚
  - å¦‚æœè®¾ç½®å®ƒçš„å±æ€§ `.requires_grad` ä¸º `True`ï¼Œé‚£ä¹ˆå®ƒå°†ä¼šè¿½è¸ªå¯¹äºè¯¥å¼ é‡çš„æ‰€æœ‰æ“ä½œã€‚
  - å½“å®Œæˆè®¡ç®—åå¯ä»¥é€šè¿‡è°ƒç”¨ `.backward()`ï¼Œæ¥è‡ªåŠ¨è®¡ç®—æ‰€æœ‰çš„æ¢¯åº¦ã€‚
  - è¿™ä¸ªå¼ é‡çš„æ‰€æœ‰æ¢¯åº¦å°†ä¼šè‡ªåŠ¨ç´¯åŠ åˆ°`.grad`å±æ€§.
- è¦é˜»æ­¢ä¸€ä¸ªå¼ é‡è¢«è·Ÿè¸ªå†å²ï¼Œå¯ä»¥è°ƒç”¨ `.detach()` æ–¹æ³•å°†å…¶ä¸è®¡ç®—å†å²åˆ†ç¦»ï¼Œå¹¶é˜»æ­¢å®ƒæœªæ¥çš„è®¡ç®—è®°å½•è¢«è·Ÿè¸ªã€‚ä¸ºäº†é˜²æ­¢è·Ÿè¸ªå†å²è®°å½•ï¼ˆå’Œä½¿ç”¨å†…å­˜ï¼‰ï¼Œå¯ä»¥å°†ä»£ç å—åŒ…è£…åœ¨ `with torch.no_grad():` ä¸­ã€‚åœ¨è¯„ä¼°æ¨¡å‹æ—¶ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºæ¨¡å‹å¯èƒ½å…·æœ‰ `requires_grad = True` çš„å¯è®­ç»ƒçš„å‚æ•°ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸éœ€è¦åœ¨æ­¤è¿‡ç¨‹ä¸­å¯¹ä»–ä»¬è¿›è¡Œæ¢¯åº¦è®¡ç®—ã€‚
- è¿˜æœ‰ä¸€ä¸ªç±»å¯¹äºautogradçš„å®ç°éå¸¸é‡è¦ï¼š`Function`ã€‚`Tensor` å’Œ `Function` äº’ç›¸è¿æ¥ç”Ÿæˆäº†ä¸€ä¸ªæ— åœˆå›¾(acyclic graph)ï¼Œå®ƒç¼–ç äº†å®Œæ•´çš„è®¡ç®—å†å²ã€‚æ¯ä¸ªå¼ é‡éƒ½æœ‰ä¸€ä¸ª `.grad_fn` å±æ€§ï¼Œè¯¥å±æ€§å¼•ç”¨äº†åˆ›å»º `Tensor` è‡ªèº«çš„`Function`ï¼ˆé™¤éè¿™ä¸ªå¼ é‡æ˜¯ç”¨æˆ·æ‰‹åŠ¨åˆ›å»ºçš„ï¼Œå³è¿™ä¸ªå¼ é‡çš„ `grad_fn` æ˜¯ `None` ï¼‰ã€‚
- å¦‚æœéœ€è¦è®¡ç®—å¯¼æ•°ï¼Œå¯ä»¥åœ¨ `Tensor` ä¸Šè°ƒç”¨ `.backward()`ã€‚å¦‚æœ `Tensor` æ˜¯ä¸€ä¸ªæ ‡é‡ï¼ˆå³å®ƒåŒ…å«ä¸€ä¸ªå…ƒç´ çš„æ•°æ®ï¼‰ï¼Œåˆ™ä¸éœ€è¦ä¸º `backward()` æŒ‡å®šä»»ä½•å‚æ•°ï¼Œä½†æ˜¯å¦‚æœå®ƒæœ‰æ›´å¤šçš„å…ƒç´ ï¼Œåˆ™éœ€è¦æŒ‡å®šä¸€ä¸ª `gradient` å‚æ•°ï¼Œè¯¥å‚æ•°æ˜¯å½¢çŠ¶åŒ¹é…çš„å¼ é‡ã€‚

```python
import torch

# åˆ›å»ºä¸€ä¸ªå¼ é‡å¹¶è®¾ç½®requires_grad=Trueç”¨æ¥è¿½è¸ªå…¶è®¡ç®—å†å²
x = torch.ones(2,2,requires_grad=True)
print(x)
'''
è¾“å‡º
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
'''

# å¯¹å¼ é‡åšä¸€æ¬¡è¿ç®—
y = x + 2
print(y)
'''
è¾“å‡º
tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward0>)
yæ˜¯è®¡ç®—çš„ç»“æœï¼Œæ‰€ä»¥å®ƒæœ‰grad_fnå±æ€§ã€‚
'''

print(y.grad_fn)
# è¾“å‡º <AddBackward0 object at 0x7f1b248453c8>

# å¯¹yè¿›è¡Œæ›´å¤šæ“ä½œ
z = y * y * 3
out = z.mean() # å¯¹æ‰€æœ‰å…ƒç´ æ±‚å‡å€¼
print(z, out)

# è¾“å‡º
tensor([[27., 27.],
        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)


# .requires_grad_(...) åŸåœ°æ”¹å˜äº†ç°æœ‰å¼ é‡çš„ requires_grad æ ‡å¿—ã€‚å¦‚æœæ²¡æœ‰æŒ‡å®šçš„è¯ï¼Œé»˜è®¤è¾“å…¥çš„è¿™ä¸ªæ ‡å¿—æ˜¯ False
a = torch.randn(2, 2)
a = ((a * 3) / (a - 1))
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)

# è¾“å‡º
False
True
<SumBackward0 object at 0x7f1b24845f98>
```

#### æ¢¯åº¦

- åå‘ä¼ æ’­ï¼Œå› ä¸º `out` æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œå› æ­¤ `out.backward()` å’Œ `out.backward(torch.tensor(1.))` ç­‰ä»·ã€‚

```python
out.backward() # è‡ªåŠ¨è®¡ç®—æ‰€æœ‰çš„æ¢¯åº¦

# è¾“å‡ºå¯¼æ•° d(out)/dx
print(x.grad)

# è¾“å‡º
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
```



### ç¥ç»ç½‘ç»œ

- ä½¿ç”¨`torch.nn`åŒ…æ¥æ„å»ºç¥ç»ç½‘ç»œã€‚
- æˆ‘ä»¬å·²ç»å­¦ä¹ äº†`autograd`ï¼Œ`nn`åŒ…åˆ™ä¾èµ–äº`autograd`åŒ…æ¥å®šä¹‰æ¨¡å‹å¹¶å¯¹å®ƒä»¬æ±‚å¯¼ã€‚ä¸€ä¸ª`nn.Module`åŒ…å«å„ä¸ªå±‚å’Œä¸€ä¸ª`forward(input)`æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è¿”å›`output`ã€‚
- ä¸€ä¸ªç¥ç»ç½‘ç»œçš„å…¸å‹è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹ï¼š
  - å®šä¹‰åŒ…å«ä¸€äº›å¯å­¦ä¹ å‚æ•°ï¼ˆæˆ–è€…å«æƒé‡ï¼‰çš„ç¥ç»ç½‘ç»œ
  - åœ¨è¾“å…¥æ•°æ®é›†ä¸Šè¿­ä»£
  - é€šè¿‡ç½‘ç»œå¤„ç†è¾“å…¥
  - è®¡ç®—æŸå¤±ï¼ˆè¾“å‡ºå’Œæ­£ç¡®ç­”æ¡ˆçš„è·ç¦»ï¼‰
  - å°†æ¢¯åº¦åå‘ä¼ æ’­ç»™ç½‘ç»œçš„å‚æ•°
  - æ›´æ–°ç½‘ç»œçš„æƒé‡ï¼Œä¸€èˆ¬ä½¿ç”¨ä¸€ä¸ªç®€å•çš„è§„åˆ™ï¼š`weight = weight - learning_rate * gradient`

#### å®šä¹‰ç½‘ç»œ

- å®šä¹‰è¿™æ ·ä¸€ä¸ªç½‘ç»œï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
  def __init__(self):
    super(Net,self).__init__()
    # è¾“å…¥å›¾åƒchannelï¼š1ï¼›è¾“å‡ºchannelï¼š6ï¼›5x5å·ç§¯æ ¸
    self.conv1 = nn.Conv2d(1, 6, 5)
    self.conv2 = nn.Conv2d(6, 16, 5)
    # an affine operation: y = Wx + b
    self.fc1 = nn.Linear(16 * 5 * 5, 120)
    self.fc2 = nn.Linear(120, 84)
    self.fc3 = nn.Linear(84, 10)
	
  def forward(self, x):
    # 2x2 Max pooling
    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
    # å¦‚æœæ˜¯æ–¹é˜µ,åˆ™å¯ä»¥åªä½¿ç”¨ä¸€ä¸ªæ•°å­—è¿›è¡Œå®šä¹‰
    x = F.max_pool2d(F.relu(self.conv2(x)), 2)
    x = x.view(-1, self.num_flat_features(x)) # è§ä¸‹é¢è¯¥å‡½æ•°
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = self.fc3(x)
    return x
  
	def num_flat_features(self, x):
    size = x.size()[1:]  # é™¤å»batch_sizeçš„å…¶ä»–æ‰€æœ‰ç»´åº¦,pytorchä¸­ä¸º[batch_size,channle,h,w]
    num_features = 1
    for s in size:
    	num_features *= s
    return num_features
net = Net()
print(net)

# è¾“å‡º
Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)

```

- æˆ‘ä»¬åªéœ€è¦å®šä¹‰ `forward` å‡½æ•°ï¼Œ`backward`å‡½æ•°ä¼šåœ¨ä½¿ç”¨`autograd`æ—¶è‡ªåŠ¨å®šä¹‰ï¼Œ`backward`å‡½æ•°ç”¨æ¥è®¡ç®—å¯¼æ•°ã€‚å¯ä»¥åœ¨ `forward` å‡½æ•°ä¸­ä½¿ç”¨ä»»ä½•é’ˆå¯¹å¼ é‡çš„æ“ä½œå’Œè®¡ç®—ã€‚
- ä¸€ä¸ªæ¨¡å‹çš„å¯å­¦ä¹ å‚æ•°å¯ä»¥é€šè¿‡`net.parameters()`è¿”å›

```python
params = list(net.parameters())
print(len(params))
print(params[0].size())  # conv1's .weight

# è¾“å‡º
10
torch.Size([6, 1, 5, 5])

```

- å°è¯•ä¸€ä¸ªéšæœºçš„32x32çš„è¾“å…¥ï¼š

```python
input = torch.randn(1, 1, 32, 32) # 1åˆ†åˆ«ä»£è¡¨batch_size,channle
out = net(input)
print(out)
# è¾“å‡ºï¼šä¸Šç½‘ç»œç»“æœæœ€åè¾“å‡º10ä¸ªåˆ†ç±»
tensor([[ 0.0399, -0.0856,  0.0668,  0.0915,  0.0453, -0.0680, -0.1024,  0.0493,
         -0.1043, -0.1267]], grad_fn=<AddmmBackward>)

```

- æ¸…é›¶æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ç¼“å­˜ï¼Œç„¶åè¿›è¡Œéšæœºæ¢¯åº¦çš„åå‘ä¼ æ’­ï¼š

```python
net.zero_grad()
out.backward(torch.randn(1, 10))

```

> æ³¨æ„ï¼š
>
> `torch.nn`åªæ”¯æŒå°æ‰¹é‡å¤„ç†ï¼ˆmini-batchesï¼‰ã€‚æ•´ä¸ª`torch.nn`åŒ…åªæ”¯æŒå°æ‰¹é‡æ ·æœ¬çš„è¾“å…¥ï¼Œä¸æ”¯æŒå•ä¸ªæ ·æœ¬ã€‚
>
> æ¯”å¦‚ï¼Œ`nn.Conv2d` æ¥å—ä¸€ä¸ª4ç»´çš„å¼ é‡ï¼Œå³`nSamples x nChannels x Height x Width`
>
> å¦‚æœæ˜¯ä¸€ä¸ªå•ç‹¬çš„æ ·æœ¬ï¼Œåªéœ€è¦ä½¿ç”¨`input.unsqueeze(0)`æ¥æ·»åŠ ä¸€ä¸ªâ€œå‡çš„â€æ‰¹å¤§å°ç»´åº¦ã€‚

#### æŸå¤±å‡½æ•°

- ä¸€ä¸ªæŸå¤±å‡½æ•°æ¥å—ä¸€å¯¹(output, target)ä½œä¸ºè¾“å…¥ï¼Œè®¡ç®—ä¸€ä¸ªå€¼æ¥ä¼°è®¡ç½‘ç»œçš„è¾“å‡ºå’Œç›®æ ‡å€¼ç›¸å·®å¤šå°‘ã€‚
- nnåŒ…ä¸­æœ‰å¾ˆå¤šä¸åŒçš„[æŸå¤±å‡½æ•°](https://pytorch.org/docs/stable/nn.html)ã€‚`nn.MSELoss`æ˜¯æ¯”è¾ƒç®€å•çš„ä¸€ç§ï¼Œå®ƒè®¡ç®—è¾“å‡ºå’Œç›®æ ‡çš„å‡æ–¹è¯¯å·®ï¼ˆmean-squared errorï¼‰ã€‚

```python
output = net(input)
target = torch.randn(10)     # æœ¬ä¾‹å­ä¸­ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
target = target.view(1, -1)  # ä½¿ç›®æ ‡å€¼ä¸æ•°æ®å€¼å½¢çŠ¶ä¸€è‡´
criterion = nn.MSELoss()     # å‡æ–¹è¯¯å·®å‡½æ•°

loss = criterion(output, target)
print(loss)

# è¾“å‡º
tensor(1.0263, grad_fn=<MseLossBackward>)

```

- ç°åœ¨ï¼Œå¦‚æœä½¿ç”¨`loss`çš„`.grad_fn`å±æ€§è·Ÿè¸ªåå‘ä¼ æ’­è¿‡ç¨‹ï¼Œä¼šçœ‹åˆ°è®¡ç®—å›¾å¦‚ä¸‹ï¼š

```python
input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d
      -> view -> linear -> relu -> linear -> relu -> linear
      -> MSELoss
      -> loss

```

- æ‰€ä»¥ï¼Œå½“æˆ‘ä»¬è°ƒç”¨`loss.backward()`ï¼Œæ•´å¼ å›¾å¼€å§‹å…³äºlosså¾®åˆ†ï¼Œå›¾ä¸­æ‰€æœ‰è®¾ç½®äº†`requires_grad=True`çš„å¼ é‡çš„`.grad`å±æ€§ç´¯ç§¯ç€æ¢¯åº¦å¼ é‡ã€‚

#### åå‘ä¼ æ’­

- æˆ‘ä»¬åªéœ€è¦è°ƒç”¨`loss.backward()`æ¥åå‘ä¼ æ’­æƒé‡ã€‚æˆ‘ä»¬éœ€è¦æ¸…é›¶ç°æœ‰çš„æ¢¯åº¦ï¼Œå¦åˆ™æ¢¯åº¦å°†ä¼šä¸å·²æœ‰çš„æ¢¯åº¦ç´¯åŠ ã€‚
- ç°åœ¨ï¼Œæˆ‘ä»¬å°†è°ƒç”¨`loss.backward()`ï¼Œå¹¶æŸ¥çœ‹conv1å±‚çš„åç½®ï¼ˆbiasï¼‰åœ¨åå‘ä¼ æ’­å‰åçš„æ¢¯åº¦ã€‚

```python
net.zero_grad()     # æ¸…é›¶æ‰€æœ‰å‚æ•°ï¼ˆparameterï¼‰çš„æ¢¯åº¦ç¼“å­˜

print('conv1.bias.grad before backward')
print(net.conv1.bias.grad)

loss.backward()

print('conv1.bias.grad after backward')
print(net.conv1.bias.grad)

# è¾“å‡º
conv1.bias.grad before backward
tensor([0., 0., 0., 0., 0., 0.])
conv1.bias.grad after backward
tensor([ 0.0084,  0.0019, -0.0179, -0.0212,  0.0067, -0.0096])

```

#### æ›´æ–°æƒé‡

- æœ€ç®€å•çš„æ›´æ–°è§„åˆ™æ˜¯éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆSGDï¼‰:

- ```python
  weight = weight - learning_rate * gradient
  
  ```

- åœ¨ä½¿ç”¨ç¥ç»ç½‘ç»œæ—¶ï¼Œå¯èƒ½å¸Œæœ›ä½¿ç”¨å„ç§ä¸åŒçš„æ›´æ–°è§„åˆ™ï¼Œå¦‚SGDã€Nesterov-SGDã€Adamã€RMSPropç­‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªè¾ƒå°çš„åŒ…`torch.optim`ï¼Œå®ƒå®ç°äº†æ‰€æœ‰çš„è¿™äº›æ–¹æ³•ã€‚ä½¿ç”¨å®ƒå¾ˆç®€å•ï¼š

```python
import torch.optim as optim

# åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆoptimizerï¼‰
optimizer = optim.SGD(net.parameters(), lr=0.01)

# åœ¨è®­ç»ƒçš„è¿­ä»£ä¸­ï¼š
optimizer.zero_grad()   # æ¸…é›¶æ¢¯åº¦ç¼“å­˜
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()    # æ›´æ–°å‚æ•°

```

